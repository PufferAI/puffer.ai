<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>PufferLib Docs</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <main class="content">
        <nav class="nav-box">
            <ul>
                <li><a href="#post-1">Installation</a></li>
                <li><a href="#post-2">Training Demo</a></li>
                <li><a href="#post-3">Vectorization</a></li>
                <li><a href="#post-4">Emulation</a></li>
                <li><a href="#post-5">Policies</a></li>
                <li><a href="#post-6">Ocean Environments</a></li>
                <li><a href="#post-7">Third Party Environments</a></li>
            </ul>
        </nav>

        <p>PufferLib is a library for sane and simple reinforcement learning at millions of steps per second. Our key features are:</p>
                <ul>
                    <li><strong>Ocean:</strong> 20+ environments from simple arcade games to massively multiagent sims</li>
                    <li><strong>PuffeRL:</strong> Train at millions of steps/second with our algoritm in a single ~1000 line script</li>
                    <li><strong>Protein:</strong> Our algorithm for automatic hyperparameter and reward tuning</li>
                    <li><strong>Vectorization:</strong> Ultra fast synchronous and asynchronous parallel simulation</li>
                    <li><strong>Emulation:</strong> Use Gymnasium and PettingZoo environments with PufferLib</li>
                </ul>

                <p>These docs will get you started. Join the <a href="https://discord.gg/puffer" target="_blank">Discord</a> to get help, or have your questions answered instantly whenever the dev stream below is live. If you're new to RL, building and contributing a new env is the best way to learn, and we review PRs live.</p>
            </header>
        </article>

        <center>
            <iframe width="854" height="480" src="https://www.youtube.com/embed/live_stream?channel=UCl7NA-PqHS2E0F29I6wJIXA" frameborder="0" allowfullscreen></iframe>
        </center>

        <article id="post-1" class="blog-post">
            <header class="post-header">
                <h1>Installation</h1>
            </header>

            <h2>Pip</h2>

            <ul>
                <li><strong>Install nvcc</strong> for faster training. We ship a custom advantage function kernel, and you'll only get the CPU version without it.</li>
                <li><strong>Use <span style="color:cyan">--no-build-isolation</span></strong> to prevent pip from fetching a different Torch version during kernel compilation.
                <li><strong>Omit <span style="color:cyan">[train]</span></strong> if you just want our environments/wrappers without PyTorch</li>
                <li><strong>Install environments</strong> with <span style="color:cyan">[atari,procgen]</span> etc. Ocean is included by default.</li>
                <li><strong><span style="color:red">Avoid Conda</span></strong> because it builds slow env code. Use UV if you really want venvs instead of containers.</li>
            </ul>

            <pre><code>pip install pufferlib[train] --no-build-isolation</code></pre>

            <h2>Docker</h2>
            <p><a href="https://github.com/pufferai/puffertank" target="_blank">PufferTank</a> is a prebuilt GPU Docker image for PufferLib. We use it for all our dev. VSCode users can install the Dev Container plugin + Docker Desktop and just open the repo. You can also set up with CLI as below. Neovim (btw) is preinstalled.</p>
            <pre><code>git clone https://github.com/pufferai/puffertank
cd puffertank
./docker.sh test</code></pre>

        </article>

        <article id="post-2" class="blog-post">
            <header class="post-header">
                <h1>Cheat Sheet</h1>
            </header>
            <pre><code>puffer [train|eval|sweep] env_name [OPTIONS]                      # PufferLib CLI, available from package
python -m pufferlib.pufferl [train|eval|sweep] env_name [OPTIONS] # Equivalent command from source

puffer train puffer_breakout --help                               # Get help on a specific environment
puffer train puffer_breakout --train.learning-rate 0.001          # Set train params
puffer train puffer_breakout --env.vision 3                       # Set env params
puffer train puffer_breakout --vec.backend Serial                 # Set vec params
puffer train puffer_breakout --neptune --tag tag_name             # Track with Neptune
puffer train puffer_breakout --wandb --tag tag_name               # Track with Weights and Biases

puffer eval puffer_breakout                                       # Render the env with a random agent
puffer eval puffer_breakout --load-model-path path/model.pt       # Load a trained model
puffer eval puffer_breakout --load-model-path latest              # Load the latest model (ls -lt experiments/*.pt | head -n 1)
puffer eval puffer_breakout --neptune --load-id id                # Load a model from Neptune
puffer eval puffer_breakout --wandb --load-id id                  # Load a model from WandB

puffer sweep puffer_breakout --neptune --tag tag_name             # Run a hyperparameter sweep tracked with Neptune
puffer sweep puffer_breakout --wandb --tag tag_name               # Run a hyperparameter sweep tracked with Weights and Biases

# TODO: Add DDP Example


# PuffeRL
pufferl.PuffeRL(config, vecenv, policy)                           # PuffeRL trainer
PuffeRL.evaluate()                                                # Collect batch_size environment interactions
PuffeRL.train()                                                   # On one batch
PuffeRL.mean_and_log()                                            # Aggregate logs and send to Wandb/Neptune
PuffeRL.close()                                                   # Final logs and close envs
PuffeRL.save_checkpoint()
PuffeRL.print_dashboard()                                         # Monitor stats in your term

PuffeRL.NoLogger
PuffeRL.NeptuneLogger
PuffeRL.WandbLogger
logger.log(logs, step)
logger.close(model_path)
logger.download()

pufferl.train(env_name, args=None, vecenv=None, policy=None, logger=None)
pufferl.eval(env_name, args=None, vecenv=None, policy=None)
pufferl.sweep(env_name, args=None)

pufferl.load_config(env_name)                                     # Load args from cli + config file
pufferl.load_env(env_name, args)
pufferl.load_policy(args, vecenv)


# VecEnv
pufferlib.vector.make(env_creator_or_creators, env_args=None, env_kwargs=None, backend=PufferEnv, num_envs=1, seed=0, **kwargs)
vecenv.num_envs                                                   # Number of observations returned by step() and recv()
vecenv.reset()                                                    # Synchronous reset
vecenv.async_reset(seed=None)                                     # Asynchronous reset
vecenv.step(actions)                                              # Synchronous step
vecenv.send(actions)                                              # Asynchronous step - send actions to env
vecenv.recv()                                                     # Asynchronous step - receive observations from env
vecenv.notify()                                                   # Call env.notify for all envs
vecenv.close()                                                    # Close all envs


# Emulation
pufferlib.emulation.GymnasiumPufferEnv(env=None, env_creator=None, env_args=[], env_kwargs={}, buf=None, seed=0)
env.render_mode                                                   # Gymnasium render mode
env.observation_space                                             # Gymnasium/PettingZoo observation space
env.action_space                                                  # Gymnasium/PettingZoo action space
env.single_observation_space                                      # Observation space for a single agent
env.single_action_space                                           # Action space for a single agent
env.seed(seed)                                                    # Legacy Gym seed method
env.reset(seed=None)                                              # Gym/Gymnasium/PettingZoo reset
env.step(action)                                                  # Gym/Gymnasium/PettingZoo step
env.render()                                                      # Gym/Gymnasium/PettingZoo render
env.close()                                                       # Gym/Gymnasium/PettingZoo close

pufferlib.emulation.PettingZooPufferEnv(env=None, env_creator=None, env_args=[], env_kwargs={}, buf=None, seed=0)
env.agents                                                        # List of agent ids
env.possible_agents                                               # List of possible agent ids
env.done                                                          # True if all agents are done
</code></pre>
        </article>
        <article id="post-2" class="blog-post">
            <header class="post-header">
                <h1>Examples</h1>
            </header>

            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/gym_env.py">Gym:</a> Wrapping a legacy Gym environment for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/gymnasium_env.py">Gymnasium:</a> Wrapping a Gymnasium environment for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/pettingzoo_env.py">PettingZoo:</a> Wrapping a multiagent PettingZoo environment for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/puffer_env.py">PufferEnv:</a> Minimal example of our native vector env format that supports single-agent and multi-agent environments</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/structured_env.py">Structured Spaces:</a> Using PufferLib's emulation to handle non-flat observation and action spaces</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/vectorization.py">Vectorization: </a>Fast and broadly compatible serial, multiprocessed synchronous, and multiprocessed asynchronous environment simulation</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/squared">Custom PufferEnv (Squared):</a> A sample environment with implementations in C and Python. See the tutorial below for a walkthrough</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/target">Custom PufferEnv (Target):</a> A slightly more complex, multi-agent sample environment in C. Reference this after reading the Squared walkthrough.</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/target">PufferEnv Template:</a> Minimal working environment. Copy this as a starting point for your new environment.</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/examples/pufferl.py">PuffeRL (importable):</a> Minimal example importing + using PuffeRL. Allows for custom models and environments without editing our training algorithm.</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/pufferl.py">PuffeRL:</a> Our main training code for PufferLib. Inspired by CleanRL - use this as a starting point for your own algo research</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/environments/atari/environment.py">Atari Wrapper:</a> How we wrap Atari for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/environments/nethack/environment.py">NetHack Wrapper:</a> How we wrap NetHack for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/environments/procgen/environment.py">Procgen Wrapper:</a> How we wrap Procgen for use with PufferLib</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/models.py">Our Default Model:</a> Flattens observations and handles discrete, multi-discrete, and continuous action spaces. Meant as a broadly compatible starting point.</p>
            <p><a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/torch.py">Custom Models:</a> A collection of architectures used for our Ocean environments
        </article>

       <article id="post-3" class="blog-post">
           <header class="post-header">
               <h1>About PufferLib</h1>
           </header>

               <p>PufferLib is the reinforcement learning library I wish existed during my PhD. It started as a compatibility layer to make working with complex environments a breeze. Now, it's a high-performance toolkit for research and industry with optimized parallel simulation, training at millions of steps per second, and core algorithm improvements from our own research.</p>

                <h2>Emulation</h2>
                <p>Complex environments may have heirarchical observations and actions, variable numbers of agents, and other quirks that make them difficult to work with and incompatible with standard reinforcement learning libraries. This was the initial motivation for PufferLib. The idea is to make every environment <i>emulate</i> the structure of simpler environments that most original RL tooling was written for. We flatten Gym/Gymnasium/PettingZoo spaces in our wrappers and unflatten them just in time for the model forward pass. This means you don't have to keep track of structured data during vectorization, in your experience buffers, or anywhere else, allowing for simpler and faster code. We also apply extra compatibility checks and pad multiagent environments to a fixed agent count. Under the hood, our wrappers maintain the Gym/Gymnasium/PettingZoo API using a stricter subset of available spaces, so you can still use them with other libraries outside of PufferLib.</p>
 
                <h2>Vectorization</h2>
                <p>In RL, vectorization refers to the process of simulating multiple copies of an environment in parallel. Our Multiprocessing backend is fast -- much faster than Gymnasium's in most cases. Atari runs 50-60% faster synchronous and 5x faster async by our latest benchmark, and some environments like NetHack can be 10x faster even synchronous, with no API changes. Our vectorization works on almost any Gym/Gymnasium/PettingZoo environment after applying our 1-line emulation wrapper. PufferLib outperforms other vectorization implementations using the following optimizations:</p>
            <ul>
                <li><strong>A Python implementation of EnvPool.</strong> Simulates more envs than are needed per batch and returns batches of observations as soon as they are ready. Requires using the async send/recv instead of the sync step API.</li>
                <li><strong>Multiple environments per worker.</strong> Important for fast environments.</li>
                <li><strong>Shared memory.</strong> Unlike Gymnasium's implementation, we use a single buffer that is shared across environments.</li>
                <li><strong>Shared flags.</strong> Workers busy-wait on an unlocked flag instead of signaling via pipes or queues. This virtually eliminates interprocess communication overhead. Pipes are used once per episode to communicate aggregated infos.</li>
                <li><strong>Zero-copy batching.</strong> Because we use a single buffer for shared memory, we can return observations from contiguous subsets of workers without ever copying observations. Only does not work for full-async mode.</li>
                <li><strong>Native multiagent support.</strong> It's not an extra wrapper or slow bolt-on feature. PufferLib treats single-agent and multi-agent environments the same. API differences are handled at the emulation level.</li>
            </ul>

        <h2>PuffeRL</h2>
        <p>PuffeRL is our training algorithm. It's based on CleanRL's PPO + LSTM but with a ton of improvements, including core algorithm changes based on our own research. We have no plans to ship a collection of standard implementations. Instead, our goal is to continue improving our one core algorithm through research and rigorous testing on all of Ocean. See our Blog for research updates!</p>

        <h2>Policies</h2>
        <p>We don't have a policy API. You just write normal PyTorch code. We do provide some defaults and surrounding tools:</p>
        <ul>
            <li><strong>PyTorch observation unflattening:</strong> A batched inversion function for the flattening applied to structured observation spaces by our emulation layer</li>
            <li><strong>Default policies:</strong> A small collection of broadly useful networks. These include MLPs and CNNs. You can also refer to all of the models used as defaults by our Ocean environments.</li>
            <li><strong>LSTM Integration:</strong> Break your <code>forward()</code> function into <code>encode_observations()</code> and <code>decode_actions()</code> and our LSTM wrapper will handle recurrance for you. We have a major optimization here that uses LSTMCell during rollouts and LSTM during training for up to 3x faster inference.</li>
        </ul>

</article>

    </article>
    <article id="post-6" class="blog-post">
        <header class="post-header">
            <h1>Writing Your Own 1M+ SPS Environment</h1>
        </header>

        <p>Ocean environments are written with the PufferEnv API. It's a vector format very similar to Gymnasium's VectorEnv, but with native multiagent support. Writing your own is a great way to learn and a great way to contribute to PufferLib. PR something cool to have it reviewed live on stream! Here's a pure Python environment using this format: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/pysquared/pysquared.py">pysquared.py</a></p>

        <p> If you're familiar with Gymnasium/PettingZoo, you'll notice that this is almost identical. The main difference is that observations, actions, rewards, etc. are initialized from this <i>buf</i> object. All operations happen in-place to avoid creating and copying redundant arrays. Our vectorization passes in slices of shared memory during multiprocessing, so your environment is storing observations directly to a batch on the main process. Note that this means calling <i>step</i> again will overwrite your observations etc. from the previous call.</p>

        <p>The above environment runs at several hundred thousand steps/second, but pure Python quickly becomes a bottleneck as environments become more complex. Below is the exact same code written in C that runs over 100M sps: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/squared/squared.h">squared.h</a></p>

        <p>Using our standard 2-core double-buffered multiprocessing settings, this environment trains at 4M steps/second while the pure Python version only trains at 400k. There are a few more steps to bind to PufferLib, though. Add a main in a separate .c so you can demo the code and work in C locally without having to compile through Python: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/squared/squared.c">squared.c</a></p>

        <p>Now you have to bind to Python. We provide a short Python C API wrapper for this. All you have to do is write a short binding file that passes initialization args from Python and logs back from C: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/squared/binding.c">binding.c</a></p>

        <p>You can now compile the environment as a Python C extension. If you are using a fork of PufferLib, setup.py will automatically look for <i>binding.c</i>. Ensure you get the latest build with:</p>

        <pre><code>python setup.py build_ext --inplace --force</code></pre>

        <p>To use your new environment from Python, set it up as a PufferEnv: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/squared/squared.py">squared.py</a></p>

        <p>And finally, if you want to expose your environment to our trainer, add a line to <i>ocean/environment.py</i> and add a .ini file for your environment to <i>pufferlib/config/ocean</i>: <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/config/ocean/squared.ini">squared.ini</a></p>

        <p>Your environment is now available just like any other Ocean environment:</p>

        <pre><code>puffer train puffer_squared</code></pre>

        <p> We suggest writing your whole environment in C first. Compile with script/build_ocean.sh. <env_name> local to enable address sanitizer. This will catch most indexing and overflow bugs. If your env works in C but not when you bind to Python, you can usually get a stack track as follows:</p>

        <pre><code>DEBUG=1 python setup.py build_ext --inplace --force
CUDA_VISIBLE_DEVICES=None LD_PRELOAD=$(gcc -print-file-name=libasan.so) python3.12 -m pufferlib.pufferl train <env_name> --train.device cpu --vec.backend Serial</code></pre>

        <p>You do have to have your compiler and asan set up correctly. This is done for you in PufferTank. You can also set breakpoints from Python into C with gdb:</p>

        <pre><code>gdb --args python3.12 -m pufferlib.pufferl train <env_name> --train.device cpu --vec.backend Serial</code></pre>

        <p>Here's a checklist of common bugs if your env is not training:</p>
        <ul>
            <li><b>Zero or incorrect observations/actions:</b> Ensure the data type and shapes used in the Python spaces and observation/action buffers match the types you've defined in C</li>
            <li><b>Incorrect or missing resets:</b> Your environment should handle its own resets internally. For envs that never reset, it is often useful to respawn agents if they are stuck (e.g. no reward for 500 steps)</li>
            <li><b>Not zeroing reward or terminals:</b> If you don't zero out rewards, they will retain their value from the previous step. Ditto for terminals. Zero them at the start of c_step to be safe. A single memset will do it for multiagent envs</li>
            <li><b>Manually inspect data scale:</b> You want observations and rewards to be roughly in the range of -1 to 1. </li>
            <li><b>Incorrect binding args:</b> Ensure your binding sets the same args as your .c file does. Call your init function if you have one.</b>
        </ul>

        <p>There's also second tutorial environment called <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/target">Target</a>. It's multi-agent with a continuous state space and multi-discrete actions. It's commented like Squared, which is also in the repo. Star on your way in to support! When you're ready to write your own environment, you can use this <a href="https://github.com/PufferAI/PufferLib/blob/3.0/pufferlib/ocean/target">template</a> as a starting point. Set it up as follows:</p>

        <pre><code>cp -r pufferlib/ocean/template pufferlib/ocean/your_env_name
grep -rin --include=*.{h,c,py} template pufferlib/ocean/template</code></pre>

        <p>Replace instances of "template" and "Template" shown with your environment name. Also copy the .ini file from pufferlib/config/ocean/template.ini and add a line for your environment to pufferlib/ocean/environment.py. You can set up outside of PufferLib, but this lets you use our extension compilation.</p>


    </article>
            <article id="post-7" class="blog-post">
                <header class="post-header">
                    <h1>FAQ</h1>
                </header>

                <p><b>Why is it called PufferLib?</b> Would you have rathered yet another minimal tech logo? Here, have a pufferfish üê°
                <p><b>How do I export gifs?</b> We have this build in to the demo for most envs. There's not an easy way to get frames from Ocean envs, but you can use F12 for screenshots and control+F12 to record a gif.
                <p><b>Multiprocessing erroring/hanging on Mac:</b> Your OS doesn't like to run subprocesses without __main__

            <article id="post-7" class="blog-post">
                <header class="post-header">
                    <h1>Third Party Environments</h1>
                </header>

                <p>You can use any well-behaved environment with PufferLib via a 1-line wrapper. These are just the environments we have gotten around to binding manually. A lot of environments are not well behaved and subtly deviate from the Gymnasium/PettingZoo API, or they use obscure features that most libraries are not designed to handle. Our bindings just help clean this up a bit. Plus, we add any tricky system package dependencies to PufferTank. Feel free to PR new bindings or fixes for existing ones!</p>


            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/gym" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/gym?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star OpenAI Gym" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/gym">OpenAI Gym</a> is the standard API for single-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://www.gymlibrary.dev/environments/box2d/">Box2D</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/PWhiddy/PokemonRedExperiments" target="_blank">
                        <img src="https://img.shields.io/github/stars/PWhiddy/PokemonRedExperiments?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Pokemon Red" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/PWhiddy/PokemonRedExperiments">Pokemon Red</a> is one of the original Pokemon games for gameboy. This project uses the game as an environment for reinforcement learning. We are actively supporting development on this one!</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/PettingZoo" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/PettingZoo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star PettingZoo" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://pettingzoo.farama.org">PettingZoo</a> is the standard API for multi-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://pettingzoo.farama.org/environments/butterfly/">Butterfly</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Arcade-Learning-Environment?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Arcade Learning Environment" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment">Arcade Learning Environment</a> provides a Gym interface for classic Atari games. This is the most popular benchmark for reinforcement learning algorithms.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Minigrid" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Minigrid?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Minigrid" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Minigrid">Minigrid</a> is a 2D grid-world environment engine and a collection of builtin environments. The target is flexible and computationally efficient RL research.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/geek-ai/MAgent" target="_blank">
                        <img src="https://img.shields.io/github/stars/geek-ai/MAgent?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MAgent" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md">MAgent</a> is a platform for large-scale agent simulation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/neuralmmo/environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/neural-mmo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Neural MMO" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://neuralmmo.github.io">Neural MMO</a> is a massively multiagent environment for reinforcement learning. It combines large agent populations with high per-agent complexity and is the most actively maintained (by me) project on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/procgen" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/procgen?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Procgen" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/procgen">Procgen</a> is a suite of arcade games for reinforcement learning with procedurally generated levels. It is one of the most computationally efficient environments on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/nle" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/nle?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star NLE" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">Nethack Learning Environment</a> is a port of the classic game NetHack to the Gym API. It combines extreme complexity with high simulation efficiency.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/minihack" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/minihack?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MiniHack" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">MiniHack Learning Environment</a> is a stripped down version of NetHack with support for level editing and custom procedural generation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/danijar/crafter" target="_blank">
                        <img src="https://img.shields.io/github/stars/danijar/crafter?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Crafter" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/danijar/crafter">Crafter</a> is a top-down 2D Minecraft clone for RL research. It provides pixel observations and relatively long time horizons.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Emerge-Lab/gpudrive" target="_blank">
                        <img src="https://img.shields.io/github/stars/Emerge-Lab/gpudrive?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star GPUDrive" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Emerge-Lab/gpudrive">GPUDrive</a> GPUDrive is a GPU-accelerated, multi-agent driving simulator that runs at 1 million FPS.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Bam4d/Griddly" target="_blank">
                        <img src="https://img.shields.io/github/stars/Bam4d/Griddly?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Griddly" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://griddly.readthedocs.io/en/latest/">Griddly</a> is an extremely optimized platform for building reinforcement learning environments. It also includes a large suite of built-in environments.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/MicroRTS-Py" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/MicroRTS-Py?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MicroRTS-Py" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/MicroRTS-Py">Gym MicroRTS</a> is a real time strategy engine for reinforcement learning research. The Java configuration is a bit finicky -- we're still debugging this.</p>
                </div>
            </div>
            </article>

        </div>
    </main>
    <script src="header.js"></script>
    <link rel="stylesheet" href="highlight.css">
    <script src="highlight.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
